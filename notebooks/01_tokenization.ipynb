{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Understanding Tokenization\n",
    "\n",
    "Welcome to the first notebook in our LLM from Scratch series! In this chapter, we'll explore **tokenization** - the fundamental first step in processing text for language models.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **What is tokenization** and why it's essential for LLMs\n",
    "2. **Different tokenization approaches**: character-level, word-level, and subword-level\n",
    "3. **Byte Pair Encoding (BPE)**: the algorithm used by GPT models\n",
    "4. **Hands-on practice** with our Tokenizer implementation\n",
    "5. **Special tokens** and their purposes\n",
    "6. **Vocabulary size** and its trade-offs\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Tokenization?\n",
    "\n",
    "**Tokenization** is the process of converting raw text into a sequence of integers (token IDs) that a neural network can process.\n",
    "\n",
    "### Why do we need it?\n",
    "\n",
    "Neural networks can only process numerical data, not text directly. We need a systematic way to:\n",
    "- Convert text → numbers (encoding)\n",
    "- Convert numbers → text (decoding)\n",
    "- Handle a fixed vocabulary efficiently\n",
    "- Deal with rare or unknown words\n",
    "\n",
    "Let's visualize this process:\n",
    "\n",
    "```\n",
    "Text:  \"Hello, world!\" \n",
    "   ↓ (tokenize)\n",
    "Tokens: [\"Hello\", \",\", \" world\", \"!\"]\n",
    "   ↓ (encode)\n",
    "IDs:   [15496, 11, 995, 0]\n",
    "   ↓ (neural network)\n",
    "Output IDs: [15496, 11, 995, 0, 1374, 389, 345, 30]\n",
    "   ↓ (decode)\n",
    "Text:  \"Hello, world! How are you?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Different Tokenization Approaches\n",
    "\n",
    "There are three main approaches to tokenization:\n",
    "\n",
    "### Character-Level Tokenization\n",
    "- **Vocabulary**: Just 26 letters + punctuation (~100 tokens)\n",
    "- **Pros**: Small vocabulary, can handle any word\n",
    "- **Cons**: Very long sequences, loses word meaning\n",
    "- **Example**: \"Hello\" → [\"H\", \"e\", \"l\", \"l\", \"o\"] → [7, 4, 11, 11, 14]\n",
    "\n",
    "### Word-Level Tokenization\n",
    "- **Vocabulary**: Every unique word (~100k-1M tokens)\n",
    "- **Pros**: Preserves word meaning, shorter sequences\n",
    "- **Cons**: Huge vocabulary, can't handle unknown words\n",
    "- **Example**: \"Hello world\" → [\"Hello\", \"world\"] → [23415, 1002]\n",
    "\n",
    "### Subword-Level Tokenization (BPE)\n",
    "- **Vocabulary**: Frequently occurring subwords (~50k tokens)\n",
    "- **Pros**: Balance between vocabulary size and sequence length\n",
    "- **Cons**: Slightly more complex algorithm\n",
    "- **Example**: \"unhappiness\" → [\"un\", \"happiness\"] → [403, 12084]\n",
    "\n",
    "**Modern LLMs like GPT use subword tokenization** because it provides the best trade-off!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Byte Pair Encoding (BPE)\n",
    "\n",
    "BPE is an iterative algorithm that builds a vocabulary of subword units:\n",
    "\n",
    "### Algorithm Steps:\n",
    "\n",
    "1. **Initialize**: Start with individual characters as tokens\n",
    "2. **Count pairs**: Find the most frequent pair of adjacent tokens\n",
    "3. **Merge**: Create a new token for this pair\n",
    "4. **Repeat**: Go to step 2 until vocabulary reaches desired size\n",
    "\n",
    "### Example:\n",
    "\n",
    "```\n",
    "Corpus: \"low\", \"lower\", \"lowest\", \"newer\", \"wider\"\n",
    "\n",
    "Initial: ['l', 'o', 'w', 'e', 'r', 'n', 'i', 'd']\n",
    "\n",
    "Iteration 1: Most frequent pair is ('e', 'r')\n",
    "Merge → 'er'\n",
    "Vocabulary: ['l', 'o', 'w', 'e', 'r', 'n', 'i', 'd', 'er']\n",
    "\n",
    "Iteration 2: Most frequent pair is ('l', 'o')\n",
    "Merge → 'lo'\n",
    "Vocabulary: ['l', 'o', 'w', 'e', 'r', 'n', 'i', 'd', 'er', 'lo']\n",
    "\n",
    "... continue until vocabulary size = 50,000\n",
    "```\n",
    "\n",
    "### Key Benefits:\n",
    "\n",
    "- **Common words** become single tokens (\"the\" → [262])\n",
    "- **Rare words** are split into frequent subwords (\"unhappiness\" → [\"un\", \"happiness\"])\n",
    "- **Unknown words** can always be represented (worst case: fall back to bytes)\n",
    "- **Efficient** vocabulary size (~50k vs. millions for word-level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hands-On: Using Our Tokenizer\n",
    "\n",
    "Let's import and use the Tokenizer from our LLM implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our tokenizer\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "\n",
    "from src.llm import Tokenizer\n",
    "\n",
    "# Create a tokenizer instance (uses GPT-2's BPE encoding)\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "print(f\"Tokenizer: {tokenizer}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"End-of-sequence token ID: {tokenizer.eos_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Basic Encoding and Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode text to token IDs\n",
    "text = \"Hello, world!\"\n",
    "token_ids = tokenizer.encode(text)\n",
    "\n",
    "print(f\"Original text: {text!r}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Number of tokens: {len(token_ids)}\")\n",
    "\n",
    "# Decode back to text\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(f\"\\nDecoded text: {decoded_text!r}\")\n",
    "print(f\"Round-trip successful: {text == decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Exploring How Words are Tokenized\n",
    "\n",
    "Let's see how different words are split into tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to visualize tokenization\n",
    "def show_tokenization(text: str) -> None:\n",
    "    \"\"\"Show how text is tokenized into subwords.\"\"\"\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    \n",
    "    # Decode each token individually to see the subwords\n",
    "    tokens = [tokenizer.decode([tid]) for tid in token_ids]\n",
    "    \n",
    "    print(f\"Text: {text!r}\")\n",
    "    print(f\"Tokens ({len(tokens)}): {tokens}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    print()\n",
    "\n",
    "# Try different examples\n",
    "show_tokenization(\"Hello\")                    # Common word\n",
    "show_tokenization(\"unhappiness\")              # Rare word with prefix\n",
    "show_tokenization(\"ChatGPT\")                   # Modern term\n",
    "show_tokenization(\"supercalifragilisticexpialidocious\")  # Very rare word\n",
    "show_tokenization(\"123-456-7890\")              # Numbers and punctuation\n",
    "show_tokenization(\"こんにちは\")                # Non-English (Japanese)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "- **Common words** like \"Hello\" are often single tokens\n",
    "- **Rare words** are broken into meaningful subwords\n",
    "- **Unknown words** fall back to character or byte-level representation\n",
    "- **Spaces** are included in the tokenization (note the leading space in some tokens)\n",
    "- **Non-English** text uses byte-level encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode multiple texts at once\n",
    "texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is fascinating!\",\n",
    "    \"LLMs can generate human-like text.\"\n",
    "]\n",
    "\n",
    "batch_ids = tokenizer.encode_batch(texts)\n",
    "\n",
    "for i, (text, ids) in enumerate(zip(texts, batch_ids), 1):\n",
    "    print(f\"Text {i}: {text}\")\n",
    "    print(f\"Tokens: {len(ids)}\")\n",
    "    print(f\"IDs: {ids}\")\n",
    "    print()\n",
    "\n",
    "# Decode batch\n",
    "decoded_texts = tokenizer.decode_batch(batch_ids)\n",
    "print(f\"Decoding successful: {texts == decoded_texts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Special Tokens\n",
    "\n",
    "Tokenizers use **special tokens** to mark important positions in sequences:\n",
    "\n",
    "- **BOS (Beginning of Sequence)**: Marks the start of a text\n",
    "- **EOS (End of Sequence)**: Marks the end of a text  \n",
    "- **PAD (Padding)**: Fills sequences to the same length\n",
    "- **UNK (Unknown)**: Represents unknown tokens (rare in BPE)\n",
    "\n",
    "Note: GPT-2 uses `<|endoftext|>` for both BOS and EOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special token IDs\n",
    "print(f\"BOS token ID: {tokenizer.bos_token_id}\")\n",
    "print(f\"EOS token ID: {tokenizer.eos_token_id}\")\n",
    "\n",
    "# Encoding with special tokens\n",
    "text = \"Hello, world!\"\n",
    "ids_no_special = tokenizer.encode(text, add_special_tokens=False)\n",
    "ids_with_special = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "print(f\"\\nWithout BOS: {ids_no_special}\")\n",
    "print(f\"With BOS:    {ids_with_special}\")\n",
    "print(f\"BOS added at position 0: {ids_with_special[0] == tokenizer.bos_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Vocabulary Size Trade-offs\n",
    "\n",
    "The choice of vocabulary size has important implications:\n",
    "\n",
    "### Small Vocabulary (e.g., 1,000 tokens)\n",
    "- ✅ **Pros**: Smaller embedding matrix, less memory\n",
    "- ❌ **Cons**: Longer sequences, loses semantic meaning\n",
    "\n",
    "### Large Vocabulary (e.g., 1,000,000 tokens)\n",
    "- ✅ **Pros**: Shorter sequences, better word representation\n",
    "- ❌ **Cons**: Huge embedding matrix, rare tokens undertrained\n",
    "\n",
    "### GPT-2's Choice: 50,257 tokens\n",
    "- ⚖️ **Sweet spot** balancing all considerations\n",
    "- Most common words and subwords are single tokens\n",
    "- Rare words split into 2-3 tokens typically\n",
    "- Embeddings are manageable (~200MB for 768 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how sequence length varies with different texts\n",
    "def compare_lengths(texts: list[str]) -> None:\n",
    "    \"\"\"Compare character length vs. token length.\"\"\"\n",
    "    print(f\"{'Text':<50} {'Chars':<8} {'Tokens':<8} {'Ratio':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for text in texts:\n",
    "        tokens = tokenizer.encode(text)\n",
    "        ratio = len(text) / len(tokens) if tokens else 0\n",
    "        print(f\"{text:<50} {len(text):<8} {len(tokens):<8} {ratio:<8.2f}\")\n",
    "\n",
    "test_texts = [\n",
    "    \"Hello\",\n",
    "    \"The quick brown fox\",\n",
    "    \"I love machine learning and neural networks!\",\n",
    "    \"Tokenization is fundamental for NLP.\",\n",
    "    \"supercalifragilisticexpialidocious\",\n",
    "]\n",
    "\n",
    "compare_lengths(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "- On average, **1 token ≈ 4 characters** in English\n",
    "- Common words have better compression ratios\n",
    "- Rare/long words have worse compression ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practical Exercise\n",
    "\n",
    "Try tokenizing your own text! Experiment with:\n",
    "- Different languages\n",
    "- Code snippets\n",
    "- Special characters and emojis\n",
    "- Very long words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn! Try different texts\n",
    "my_text = \"Write your own text here!\"\n",
    "\n",
    "show_tokenization(my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "Let's recap what we've learned:\n",
    "\n",
    "1. **Tokenization converts text → numbers** so neural networks can process it\n",
    "\n",
    "2. **BPE (Byte Pair Encoding)** is the algorithm used by modern LLMs:\n",
    "   - Iteratively merges frequent character pairs\n",
    "   - Creates a vocabulary of ~50k subword tokens\n",
    "   - Balances vocabulary size and sequence length\n",
    "\n",
    "3. **Common words** become single tokens, **rare words** split into subwords\n",
    "\n",
    "4. **Special tokens** (BOS/EOS/PAD) help mark sequence boundaries\n",
    "\n",
    "5. **Vocabulary size** (~50k) is carefully chosen to balance:\n",
    "   - Model size (embedding matrix)\n",
    "   - Sequence length\n",
    "   - Semantic representation\n",
    "\n",
    "6. **Our tokenizer** uses the same BPE encoding as GPT-2/GPT-3 (tiktoken)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand how text is converted to tokens, we're ready to explore the **attention mechanism** - the core innovation that makes transformers so powerful!\n",
    "\n",
    "Continue to **Notebook 02: Attention Mechanism** →"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909) (Original BPE paper)\n",
    "- [tiktoken documentation](https://github.com/openai/tiktoken)\n",
    "- [Hugging Face Tokenizers](https://huggingface.co/docs/tokenizers/index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

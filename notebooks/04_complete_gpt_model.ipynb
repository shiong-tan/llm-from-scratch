{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: The Complete GPT Model\n",
    "\n",
    "Welcome to the fourth notebook in our LLM from Scratch series! In this chapter, we'll bring everything together and build the **complete GPT model** - a fully functional transformer language model.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Token and positional embeddings**: Converting discrete tokens to continuous vectors\n",
    "2. **Weight tying**: Sharing parameters between embeddings and output\n",
    "3. **Scaled initialization**: Ensuring stable training for deep networks\n",
    "4. **Complete forward pass**: From token IDs to next-token predictions\n",
    "5. **Loss computation**: Cross-entropy for language modeling\n",
    "6. **Model architecture**: Understanding GPT-2/GPT-3 design choices\n",
    "7. **Hands-on experimentation**: Building and using a real GPT model\n",
    "\n",
    "This is where all the pieces come together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GPT Architecture Overview\n",
    "\n",
    "The complete GPT model has the following components:\n",
    "\n",
    "```\n",
    "Input Token IDs: [15496, 11, 995, 0]  (\"Hello, world!\")\n",
    "        ↓\n",
    "┌───────────────────────────────────────┐\n",
    "│ Token Embedding (vocab_size, d_model)│\n",
    "│    + Position Embedding (max_seq, d) │\n",
    "│    + Dropout                          │\n",
    "└───────────────────────────────────────┘\n",
    "        ↓\n",
    "┌───────────────────────────────────────┐\n",
    "│ Transformer Block 1                   │\n",
    "│  - Multi-Head Attention               │\n",
    "│  - Feedforward Network                │\n",
    "└───────────────────────────────────────┘\n",
    "        ↓\n",
    "┌───────────────────────────────────────┐\n",
    "│ Transformer Block 2                   │\n",
    "└───────────────────────────────────────┘\n",
    "       ...\n",
    "┌───────────────────────────────────────┐\n",
    "│ Transformer Block N                   │\n",
    "└───────────────────────────────────────┘\n",
    "        ↓\n",
    "┌───────────────────────────────────────┐\n",
    "│ Final LayerNorm                       │\n",
    "└───────────────────────────────────────┘\n",
    "        ↓\n",
    "┌───────────────────────────────────────┐\n",
    "│ Output Projection (d_model, vocab)   │\n",
    "│ (shares weights with Token Embedding) │\n",
    "└───────────────────────────────────────┘\n",
    "        ↓\n",
    "Logits: [batch, seq_len, vocab_size]\n",
    "        ↓\n",
    "Softmax → Next Token Probabilities\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Token Embeddings\n",
    "\n",
    "**Token embeddings** convert discrete token IDs into continuous vectors.\n",
    "\n",
    "### Why We Need Them:\n",
    "\n",
    "- Neural networks process **continuous values**, not discrete IDs\n",
    "- Token ID \"1234\" has no inherent meaning (not a number to compute with)\n",
    "- Embeddings learn **semantic representations** where similar words have similar vectors\n",
    "\n",
    "### How They Work:\n",
    "\n",
    "```python\n",
    "# Embedding layer is a lookup table\n",
    "embedding = nn.Embedding(vocab_size=50257, d_model=768)\n",
    "\n",
    "# Shape: (vocab_size, d_model) = (50257, 768)\n",
    "# Each token has a 768-dimensional vector\n",
    "\n",
    "# Lookup: convert token ID → vector\n",
    "token_id = 1234\n",
    "vector = embedding(torch.tensor([token_id]))  # → (1, 768)\n",
    "```\n",
    "\n",
    "### Key Properties:\n",
    "\n",
    "- **Learnable**: Vectors are learned during training\n",
    "- **Dense**: Each dimension can be any real number\n",
    "- **Semantic**: Similar words end up with similar vectors\n",
    "- **Large**: vocab_size × d_model parameters (e.g., 50,257 × 768 = 38.5M params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Positional Embeddings\n",
    "\n",
    "**Positional embeddings** encode position information into the model.\n",
    "\n",
    "### Why We Need Them:\n",
    "\n",
    "Self-attention is **permutation invariant**:\n",
    "- \"cat sat mat\" gets same attention as \"sat cat mat\"\n",
    "- Word order is crucial for language!\n",
    "- Need to inject position information\n",
    "\n",
    "### Two Approaches:\n",
    "\n",
    "#### 1. Sinusoidal (Original Transformer)\n",
    "```python\n",
    "# Fixed mathematical formula\n",
    "PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "```\n",
    "- ✅ Can extrapolate to longer sequences\n",
    "- ❌ Fixed, cannot adapt to data\n",
    "\n",
    "#### 2. Learned (GPT-2/GPT-3)\n",
    "```python\n",
    "# Learned embedding layer\n",
    "pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "```\n",
    "- ✅ Can learn optimal encodings for the data\n",
    "- ✅ Often works better empirically\n",
    "- ❌ Limited to max_seq_len\n",
    "\n",
    "### Our Implementation (GPT-2 Style):\n",
    "\n",
    "```python\n",
    "# Token embedding: which word is it?\n",
    "token_emb = token_embedding(input_ids)  # (batch, seq_len, d_model)\n",
    "\n",
    "# Position embedding: where in the sequence?\n",
    "positions = torch.arange(seq_len)  # [0, 1, 2, ..., seq_len-1]\n",
    "pos_emb = position_embedding(positions)  # (seq_len, d_model)\n",
    "\n",
    "# Combine\n",
    "x = token_emb + pos_emb  # (batch, seq_len, d_model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Weight Tying\n",
    "\n",
    "**Weight tying** shares parameters between input embeddings and output projection.\n",
    "\n",
    "### The Idea:\n",
    "\n",
    "```python\n",
    "# WITHOUT weight tying\n",
    "token_embedding = nn.Embedding(vocab_size, d_model)  # vocab_size × d_model params\n",
    "output_projection = nn.Linear(d_model, vocab_size)   # vocab_size × d_model params\n",
    "# Total: 2 × vocab_size × d_model parameters\n",
    "\n",
    "# WITH weight tying\n",
    "token_embedding = nn.Embedding(vocab_size, d_model)  # vocab_size × d_model params\n",
    "output_projection.weight = token_embedding.weight     # SHARED!\n",
    "# Total: 1 × vocab_size × d_model parameters (50% reduction!)\n",
    "```\n",
    "\n",
    "### Why This Makes Sense:\n",
    "\n",
    "- **Input embedding**: \"Which vector represents token X?\"\n",
    "- **Output projection**: \"Which token does vector Y represent?\"\n",
    "- These are **inverse operations** - makes sense to share weights!\n",
    "\n",
    "### Benefits:\n",
    "\n",
    "1. ✅ **Reduces parameters**: 50% reduction in embedding parameters\n",
    "2. ✅ **Better generalization**: More efficient use of parameters\n",
    "3. ✅ **Improved performance**: Empirically works better\n",
    "4. ✅ **Used in all modern LLMs**: GPT-2, GPT-3, BERT, etc.\n",
    "\n",
    "See: \"Using the Output Embedding to Improve Language Models\" (Press & Wolf, 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scaled Initialization\n",
    "\n",
    "**Scaled initialization** is crucial for training deep transformers.\n",
    "\n",
    "### The Problem:\n",
    "\n",
    "In deep networks, activations can **explode** or **vanish**:\n",
    "```\n",
    "Layer 1: x has std=1.0\n",
    "Layer 2: x has std=2.0  (growing!)\n",
    "Layer 3: x has std=4.0\n",
    "Layer 96: x has std=∞  (exploded!)\n",
    "```\n",
    "\n",
    "### The Solution (GPT-2/GPT-3):\n",
    "\n",
    "Scale residual projections by $\\frac{1}{\\sqrt{2N}}$ where N = number of layers\n",
    "\n",
    "```python\n",
    "# Normal initialization\n",
    "nn.init.normal_(weight, mean=0.0, std=0.02)\n",
    "\n",
    "# Scaled initialization for residual projections\n",
    "nn.init.normal_(weight, mean=0.0, std=0.02 / sqrt(2 * n_layers))\n",
    "```\n",
    "\n",
    "### Which Layers Get Scaled?\n",
    "\n",
    "Only **residual projections**:\n",
    "1. Attention output projection (`attn.out_proj`)\n",
    "2. FFN output projection (`ffn.fc2`)\n",
    "\n",
    "These are the layers that **add** to the residual stream.\n",
    "\n",
    "### Why It Works:\n",
    "\n",
    "- At initialization, residual paths contribute **less**\n",
    "- Model starts close to **identity** (x ≈ x + 0)\n",
    "- Gradually learns to add meaningful transformations\n",
    "- Prevents activation explosion in 100+ layer networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hands-On: Building a Complete GPT Model\n",
    "\n",
    "Let's create and experiment with our GPT implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from src.llm import GPTModel, ModelConfig, Tokenizer\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Create a Small GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small GPT configuration\n",
    "config = ModelConfig(\n",
    "    vocab_size=50257,  # GPT-2 vocabulary\n",
    "    max_seq_len=256,   # Maximum sequence length\n",
    "    d_model=256,       # Model dimension\n",
    "    n_layers=6,        # Number of transformer blocks\n",
    "    n_heads=8,         # Number of attention heads\n",
    "    d_ff=1024,         # Feedforward dimension (4 × d_model)\n",
    "    dropout=0.1,       # Dropout probability\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = GPTModel(config)\n",
    "\n",
    "print(f\"Model Configuration:\")\n",
    "print(f\"  Vocabulary size: {config.vocab_size:,}\")\n",
    "print(f\"  Max sequence length: {config.max_seq_len}\")\n",
    "print(f\"  Model dimension: {config.d_model}\")\n",
    "print(f\"  Number of layers: {config.n_layers}\")\n",
    "print(f\"  Attention heads: {config.n_heads}\")\n",
    "print(f\"  FFN dimension: {config.d_ff}\")\n",
    "\n",
    "print(f\"\\nModel Statistics:\")\n",
    "total_params = model.num_parameters()\n",
    "non_emb_params = model.num_parameters(exclude_embeddings=True)\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Non-embedding parameters: {non_emb_params:,}\")\n",
    "print(f\"  Embedding parameters: {total_params - non_emb_params:,}\")\n",
    "print(f\"  Embedding %: {(total_params - non_emb_params) / total_params * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Forward Pass: From Tokens to Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Encode some text\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "input_ids = tokenizer.encode(text)\n",
    "print(f\"Input text: {text}\")\n",
    "print(f\"Token IDs: {input_ids}\")\n",
    "print(f\"Num tokens: {len(input_ids)}\")\n",
    "\n",
    "# Convert to tensor\n",
    "input_tensor = torch.tensor([input_ids])  # Add batch dimension\n",
    "print(f\"\\nInput shape: {input_tensor.shape}  # (batch_size, seq_len)\")\n",
    "\n",
    "# Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits, _ = model(input_tensor)\n",
    "\n",
    "print(f\"\\nOutput shape: {logits.shape}  # (batch_size, seq_len, vocab_size)\")\n",
    "print(f\"Logits for last position: {logits[0, -1, :5]}... (first 5 vocab items)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Next Token Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get next token probabilities\n",
    "next_token_logits = logits[0, -1, :]  # Last position\n",
    "next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "# Get top-5 most likely next tokens\n",
    "top_k = 5\n",
    "top_probs, top_indices = torch.topk(next_token_probs, top_k)\n",
    "\n",
    "print(f\"Top {top_k} most likely next tokens:\")\n",
    "print(f\"{'Token':<30} {'Token ID':<12} {'Probability':<12}\")\n",
    "print(\"-\" * 60)\n",
    "for prob, idx in zip(top_probs, top_indices):\n",
    "    token = tokenizer.decode([idx.item()])\n",
    "    print(f\"{repr(token):<30} {idx.item():<12} {prob.item():<12.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Visualizing Token and Position Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get token embeddings for our input\n",
    "with torch.no_grad():\n",
    "    token_emb = model.token_embedding(input_tensor)  # (1, seq_len, d_model)\n",
    "    \n",
    "    seq_len = input_tensor.shape[1]\n",
    "    positions = torch.arange(seq_len)\n",
    "    pos_emb = model.position_embedding(positions)  # (seq_len, d_model)\n",
    "\n",
    "# Visualize embeddings\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Token embeddings\n",
    "im1 = ax1.imshow(token_emb[0].T.numpy(), aspect='auto', cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "ax1.set_xlabel('Token Position', fontsize=12)\n",
    "ax1.set_ylabel('Embedding Dimension', fontsize=12)\n",
    "ax1.set_title('Token Embeddings', fontsize=14)\n",
    "plt.colorbar(im1, ax=ax1, label='Value')\n",
    "\n",
    "# Position embeddings\n",
    "im2 = ax2.imshow(pos_emb.T.numpy(), aspect='auto', cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "ax2.set_xlabel('Token Position', fontsize=12)\n",
    "ax2.set_ylabel('Embedding Dimension', fontsize=12)\n",
    "ax2.set_title('Position Embeddings', fontsize=14)\n",
    "plt.colorbar(im2, ax=ax2, label='Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- Token embeddings: Different patterns for each word\")\n",
    "print(\"- Position embeddings: Gradually varying pattern across positions\")\n",
    "print(\"- Combined: Both 'what' and 'where' information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Loss Computation for Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For language modeling, targets are inputs shifted by one position\n",
    "# Input:  \"The cat sat\"\n",
    "# Target: \"cat sat on\" (predict next token at each position)\n",
    "\n",
    "# Create targets (shift input by 1)\n",
    "input_ids = torch.tensor([input_ids])  # (1, seq_len)\n",
    "targets = input_ids.clone()\n",
    "\n",
    "# In practice, we shift during training\n",
    "# Input: input_ids[:, :-1]  (all except last)\n",
    "# Target: input_ids[:, 1:]  (all except first)\n",
    "\n",
    "# For this example, use same for both (not typical)\n",
    "logits, loss = model(input_ids, targets=targets, return_loss=True)\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print(f\"Perplexity: {torch.exp(loss).item():.2f}\")\n",
    "\n",
    "print(\"\\nNote: High loss/perplexity is expected for untrained model!\")\n",
    "print(\"Random predictions ≈ log(vocab_size) ≈ log(50257) ≈ 10.8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing Model Sizes\n",
    "\n",
    "Let's compare our model to famous GPT models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Famous GPT configurations\n",
    "gpt_configs = [\n",
    "    {\"name\": \"Our Tiny\", \"d_model\": 256, \"n_layers\": 6, \"n_heads\": 8},\n",
    "    {\"name\": \"GPT-2 Small\", \"d_model\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    {\"name\": \"GPT-2 Medium\", \"d_model\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    {\"name\": \"GPT-2 Large\", \"d_model\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    {\"name\": \"GPT-2 XL\", \"d_model\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "    {\"name\": \"GPT-3\", \"d_model\": 12288, \"n_layers\": 96, \"n_heads\": 96},\n",
    "]\n",
    "\n",
    "print(f\"{'Model':<20} {'d_model':<10} {'Layers':<10} {'Heads':<10} {'Parameters':<15}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for cfg in gpt_configs:\n",
    "    config = ModelConfig(\n",
    "        vocab_size=50257,\n",
    "        d_model=cfg[\"d_model\"],\n",
    "        n_layers=cfg[\"n_layers\"],\n",
    "        n_heads=cfg[\"n_heads\"],\n",
    "        d_ff=4 * cfg[\"d_model\"],  # Standard 4x expansion\n",
    "    )\n",
    "    model = GPTModel(config)\n",
    "    params = model.num_parameters()\n",
    "    \n",
    "    print(f\"{cfg['name']:<20} {cfg['d_model']:<10} {cfg['n_layers']:<10} \"\n",
    "          f\"{cfg['n_heads']:<10} {params:>12,}\")\n",
    "\n",
    "print(\"\\nNote: GPT-3 has 175 BILLION parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Text Generation with GPT\n",
    "\n",
    "Let's use our model to generate text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small model for generation\n",
    "config = ModelConfig(\n",
    "    vocab_size=50257,\n",
    "    max_seq_len=128,\n",
    "    d_model=128,\n",
    "    n_layers=4,\n",
    "    n_heads=4,\n",
    ")\n",
    "model = GPTModel(config)\n",
    "model.eval()\n",
    "\n",
    "# Input prompt\n",
    "prompt = \"Once upon a time\"\n",
    "input_ids = tokenizer.encode(prompt)\n",
    "input_tensor = torch.tensor([input_ids])\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generating...\\n\")\n",
    "\n",
    "# Generate with different temperatures\n",
    "for temperature in [0.5, 1.0, 1.5]:\n",
    "    generated = model.generate(\n",
    "        input_tensor,\n",
    "        max_new_tokens=20,\n",
    "        temperature=temperature,\n",
    "        top_k=50\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(generated[0].tolist())\n",
    "    print(f\"Temperature {temperature}:\")\n",
    "    print(f\"  {generated_text}\")\n",
    "    print()\n",
    "\n",
    "print(\"Note: Output is random/poor quality because model is untrained!\")\n",
    "print(\"After training on real data, generation quality improves dramatically.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Understanding Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze parameter distribution\n",
    "config = ModelConfig(d_model=256, n_layers=6, n_heads=8)\n",
    "model = GPTModel(config)\n",
    "\n",
    "component_params = {\n",
    "    \"Token Embedding\": sum(p.numel() for p in model.token_embedding.parameters()),\n",
    "    \"Position Embedding\": sum(p.numel() for p in model.position_embedding.parameters()),\n",
    "    \"Transformer Blocks\": sum(p.numel() for p in model.blocks.parameters()),\n",
    "    \"Final LayerNorm\": sum(p.numel() for p in model.ln_final.parameters()),\n",
    "}\n",
    "\n",
    "total = sum(component_params.values())\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart\n",
    "ax1.pie(\n",
    "    component_params.values(),\n",
    "    labels=component_params.keys(),\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90\n",
    ")\n",
    "ax1.set_title('Parameter Distribution', fontsize=14)\n",
    "\n",
    "# Bar chart\n",
    "bars = ax2.barh(list(component_params.keys()), list(component_params.values()))\n",
    "ax2.set_xlabel('Number of Parameters', fontsize=12)\n",
    "ax2.set_title('Parameters by Component', fontsize=14)\n",
    "ax2.ticklabel_format(axis='x', style='plain')\n",
    "\n",
    "# Add value labels\n",
    "for i, (component, params) in enumerate(component_params.items()):\n",
    "    ax2.text(params, i, f' {params:,}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total parameters: {total:,}\")\n",
    "print(f\"\\nNote: Transformer blocks contain most parameters (attention + FFN)\")\n",
    "print(f\"Note: lm_head shares weights with token_embedding, so not counted separately\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways\n",
    "\n",
    "Let's recap what we've learned about the complete GPT model:\n",
    "\n",
    "1. **Complete architecture = Embeddings + Transformer Blocks + Output**:\n",
    "   - Token embeddings: Convert IDs to vectors\n",
    "   - Position embeddings: Encode sequence position\n",
    "   - Transformer blocks: Process information\n",
    "   - Output projection: Predict next token\n",
    "\n",
    "2. **Embeddings are crucial**:\n",
    "   - Token embeddings learn semantic representations\n",
    "   - Position embeddings break attention's permutation invariance\n",
    "   - Combined with addition: simple and effective\n",
    "\n",
    "3. **Weight tying reduces parameters**:\n",
    "   - Share weights between input and output embeddings\n",
    "   - 50% reduction in embedding parameters\n",
    "   - Better generalization and performance\n",
    "\n",
    "4. **Scaled initialization enables deep networks**:\n",
    "   - Scale residual projections by 1/√(2N)\n",
    "   - Prevents activation explosion\n",
    "   - Essential for training 100+ layer models\n",
    "\n",
    "5. **Forward pass**: Token IDs → Embeddings → Transformer Blocks → Logits\n",
    "   - Causal masking prevents looking at future tokens\n",
    "   - Softmax converts logits to probabilities\n",
    "   - Cross-entropy loss for training\n",
    "\n",
    "6. **Model scaling**:\n",
    "   - Increase d_model, n_layers, n_heads for more capacity\n",
    "   - GPT-3: 175B parameters!\n",
    "   - Most parameters in transformer blocks and embeddings\n",
    "\n",
    "7. **Generation**:\n",
    "   - Autoregressive: one token at a time\n",
    "   - Temperature controls randomness\n",
    "   - Quality depends on training data\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that we have a complete GPT model, we're ready to learn about **training and text generation** - how to train the model on data and generate high-quality text!\n",
    "\n",
    "Continue to **Notebook 05: Training and Generation** →"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (GPT-2 paper)\n",
    "- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) (GPT-3 paper)\n",
    "- [Using the Output Embedding to Improve Language Models](https://arxiv.org/abs/1608.05859) (Weight tying)\n",
    "- [The Illustrated GPT-2](http://jalammar.github.io/illustrated-gpt2/) (Jay Alammar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

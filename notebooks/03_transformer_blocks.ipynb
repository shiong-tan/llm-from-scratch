{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Transformer Blocks\n",
    "\n",
    "Welcome to the third notebook in our LLM from Scratch series! In this chapter, we'll explore the **Transformer Block** - the fundamental building block that gets stacked to create powerful models like GPT.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Transformer block architecture**: Combining attention and feedforward networks\n",
    "2. **Residual connections**: Why they're essential for deep networks\n",
    "3. **Layer normalization**: Stabilizing training\n",
    "4. **Pre-norm vs post-norm**: Modern architecture choices\n",
    "5. **Feed forward networks**: Position-wise processing\n",
    "6. **Hands-on experimentation** with our implementation\n",
    "\n",
    "Let's build the complete transformer block!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Transformer Block Architecture\n",
    "\n",
    "A transformer block combines **two main components**:\n",
    "\n",
    "### Component 1: Multi-Head Self-Attention\n",
    "- Allows positions to communicate with each other\n",
    "- Captures relationships and dependencies\n",
    "- We learned about this in Notebook 02!\n",
    "\n",
    "### Component 2: Feedforward Network (FFN)\n",
    "- Processes each position independently\n",
    "- Adds non-linear transformations\n",
    "- Increases model capacity\n",
    "\n",
    "### The Complete Block:\n",
    "\n",
    "```\n",
    "Input\n",
    "  ↓\n",
    "LayerNorm → Multi-Head Attention → + (residual)\n",
    "  ↓                                  ↑\n",
    "  └──────────────────────────────────┘\n",
    "  ↓\n",
    "LayerNorm → FeedForward Network → + (residual)\n",
    "  ↓                                 ↑\n",
    "  └─────────────────────────────────┘\n",
    "  ↓\n",
    "Output\n",
    "```\n",
    "\n",
    "This pattern is called **Pre-LayerNorm** architecture, which is what modern models like GPT-2/GPT-3 use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Residual Connections: The Key to Deep Networks\n",
    "\n",
    "**Residual connections** (also called skip connections) allow gradients to flow directly through the network.\n",
    "\n",
    "### Without Residual Connections:\n",
    "```python\n",
    "x = layer1(x)\n",
    "x = layer2(x)\n",
    "x = layer3(x)\n",
    "# Gradient must flow through all layers → vanishing gradients!\n",
    "```\n",
    "\n",
    "### With Residual Connections:\n",
    "```python\n",
    "x = x + layer1(x)  # Residual connection\n",
    "x = x + layer2(x)  # Residual connection\n",
    "x = x + layer3(x)  # Residual connection\n",
    "# Gradient can flow directly → stable training!\n",
    "```\n",
    "\n",
    "### Why This Matters:\n",
    "\n",
    "- ✅ **Enables training deep networks** (GPT-3 has 96 layers!)\n",
    "- ✅ **Prevents vanishing gradients** (direct path for gradients)\n",
    "- ✅ **Allows learning identity function** (if needed, layer can output 0)\n",
    "- ✅ **Better optimization landscape** (easier to find good solutions)\n",
    "\n",
    "This innovation from ResNet (He et al., 2015) revolutionized deep learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Layer Normalization: Stabilizing Training\n",
    "\n",
    "**Layer normalization** normalizes activations to have mean 0 and variance 1.\n",
    "\n",
    "### Formula:\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "Where:\n",
    "- $\\mu$ = mean across features (for each example)\n",
    "- $\\sigma^2$ = variance across features (for each example)\n",
    "- $\\gamma$ = learnable scale parameter\n",
    "- $\\beta$ = learnable shift parameter\n",
    "- $\\epsilon$ = small constant for numerical stability\n",
    "\n",
    "### Why Layer Norm (not Batch Norm)?\n",
    "\n",
    "**Batch Normalization**: Normalizes across batch dimension\n",
    "- ❌ Requires large batches\n",
    "- ❌ Doesn't work well with variable-length sequences\n",
    "- ❌ Different behavior during train/test\n",
    "\n",
    "**Layer Normalization**: Normalizes across feature dimension\n",
    "- ✅ Works with any batch size\n",
    "- ✅ Perfect for sequences\n",
    "- ✅ Same behavior during train/test\n",
    "\n",
    "### Benefits:\n",
    "\n",
    "1. **Stable gradients**: Prevents exploding/vanishing gradients\n",
    "2. **Faster training**: Can use higher learning rates\n",
    "3. **Better generalization**: Acts as regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pre-Norm vs Post-Norm\n",
    "\n",
    "Two ways to arrange LayerNorm and residual connections:\n",
    "\n",
    "### Post-Norm (Original Transformer):\n",
    "```\n",
    "x = LayerNorm(x + Attention(x))\n",
    "x = LayerNorm(x + FFN(x))\n",
    "```\n",
    "\n",
    "### Pre-Norm (Modern, GPT-2/3):\n",
    "```\n",
    "x = x + Attention(LayerNorm(x))\n",
    "x = x + FFN(LayerNorm(x))\n",
    "```\n",
    "\n",
    "### Why Pre-Norm is Better:\n",
    "\n",
    "- ✅ **Better gradient flow**: Residual path doesn't go through LayerNorm\n",
    "- ✅ **More stable training**: Especially for very deep models\n",
    "- ✅ **Easier to train**: Less sensitive to hyperparameters\n",
    "- ✅ **Used in modern LLMs**: GPT-2, GPT-3, LLaMA, etc.\n",
    "\n",
    "Our implementation uses **Pre-Norm**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feedforward Network (FFN)\n",
    "\n",
    "The FFN is a simple 2-layer MLP applied **independently** to each position:\n",
    "\n",
    "### Architecture:\n",
    "```\n",
    "Input: (batch, seq_len, d_model)\n",
    "  ↓\n",
    "Linear: d_model → d_ff (expand, typically 4x)\n",
    "  ↓\n",
    "GELU activation\n",
    "  ↓\n",
    "Linear: d_ff → d_model (compress back)\n",
    "  ↓\n",
    "Dropout\n",
    "  ↓\n",
    "Output: (batch, seq_len, d_model)\n",
    "```\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "1. **Position-wise**: Same transformation applied to each position independently\n",
    "2. **Expansion**: Typically expand to 4 × d_model (e.g., 512 → 2048)\n",
    "3. **GELU activation**: Smoother than ReLU, works better for language\n",
    "4. **Adds capacity**: Provides non-linear transformations\n",
    "\n",
    "### Why GELU (not ReLU)?\n",
    "\n",
    "**ReLU**: $\\text{ReLU}(x) = \\max(0, x)$\n",
    "- Hard cutoff at 0\n",
    "- Non-smooth\n",
    "\n",
    "**GELU**: $\\text{GELU}(x) = x \\cdot \\Phi(x)$ (Φ = standard normal CDF)\n",
    "- Smooth function\n",
    "- Probabilistic interpretation\n",
    "- Better for language models empirically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hands-On: Building Transformer Blocks\n",
    "\n",
    "Let's experiment with our implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from src.llm.transformer import TransformerBlock, FeedForward\n",
    "from src.llm.attention import create_causal_mask\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Feedforward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feedforward network\n",
    "d_model = 128\n",
    "d_ff = 512  # 4 * d_model\n",
    "\n",
    "ffn = FeedForward(d_model=d_model, d_ff=d_ff, dropout=0.0)\n",
    "\n",
    "print(f\"Feedforward Network:\")\n",
    "print(f\"  Input dimension: {d_model}\")\n",
    "print(f\"  Hidden dimension: {d_ff}\")\n",
    "print(f\"  Expansion factor: {d_ff / d_model}x\")\n",
    "print(f\"\\nParameters:\")\n",
    "total_params = sum(p.numel() for p in ffn.parameters())\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output = ffn(x)\n",
    "\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Shape preserved: {x.shape == output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Visualizing GELU Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Compare ReLU and GELU\n",
    "x = torch.linspace(-3, 3, 200)\n",
    "relu = F.relu(x)\n",
    "gelu = F.gelu(x)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x.numpy(), relu.numpy(), label='ReLU', linewidth=2)\n",
    "plt.plot(x.numpy(), gelu.numpy(), label='GELU', linewidth=2)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('Input', fontsize=12)\n",
    "plt.ylabel('Output', fontsize=12)\n",
    "plt.title('ReLU vs GELU Activation Functions', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key differences:\")\n",
    "print(\"- ReLU: Hard cutoff at 0 (sharp corner)\")\n",
    "print(\"- GELU: Smooth transition (differentiable everywhere)\")\n",
    "print(\"- GELU: Allows small negative values (probabilistic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Complete Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformer block\n",
    "d_model = 128\n",
    "n_heads = 8\n",
    "d_ff = 512\n",
    "\n",
    "block = TransformerBlock(\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    d_ff=d_ff,\n",
    "    dropout=0.0  # Disable for deterministic behavior\n",
    ")\n",
    "\n",
    "print(f\"Transformer Block Configuration:\")\n",
    "print(f\"  Model dimension: {d_model}\")\n",
    "print(f\"  Attention heads: {n_heads}\")\n",
    "print(f\"  FFN dimension: {d_ff}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in block.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "# Breakdown\n",
    "attn_params = sum(p.numel() for p in block.attn.parameters())\n",
    "ffn_params = sum(p.numel() for p in block.ffn.parameters())\n",
    "ln_params = sum(p.numel() for p in block.ln1.parameters()) + sum(p.numel() for p in block.ln2.parameters())\n",
    "\n",
    "print(f\"  Attention: {attn_params:,} ({attn_params/total_params*100:.1f}%)\")\n",
    "print(f\"  Feedforward: {ffn_params:,} ({ffn_params/total_params*100:.1f}%)\")\n",
    "print(f\"  LayerNorm: {ln_params:,} ({ln_params/total_params*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Forward Pass Through Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Create causal mask\n",
    "mask = create_causal_mask(seq_len)\n",
    "\n",
    "print(f\"Input:\")\n",
    "print(f\"  Shape: {x.shape}\")\n",
    "print(f\"  Mean: {x.mean():.4f}\")\n",
    "print(f\"  Std: {x.std():.4f}\")\n",
    "\n",
    "# Forward pass\n",
    "output, attn_weights = block(x, mask=mask, return_attention=True)\n",
    "\n",
    "print(f\"\\nOutput:\")\n",
    "print(f\"  Shape: {output.shape}\")\n",
    "print(f\"  Mean: {output.mean():.4f}\")\n",
    "print(f\"  Std: {output.std():.4f}\")\n",
    "\n",
    "print(f\"\\nAttention weights:\")\n",
    "print(f\"  Shape: {attn_weights.shape}\")\n",
    "print(f\"  (batch_size, n_heads, seq_len, seq_len)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Visualizing Information Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track intermediate values through the block\n",
    "def forward_with_intermediates(block, x, mask):\n",
    "    \"\"\"Forward pass that returns intermediate values.\"\"\"\n",
    "    intermediates = {}\n",
    "    \n",
    "    # Initial input\n",
    "    intermediates['input'] = x.clone()\n",
    "    \n",
    "    # After first LayerNorm\n",
    "    ln1_out = block.ln1(x)\n",
    "    intermediates['after_ln1'] = ln1_out.clone()\n",
    "    \n",
    "    # After attention\n",
    "    attn_out, _ = block.attn(ln1_out, mask=mask)\n",
    "    intermediates['after_attn'] = attn_out.clone()\n",
    "    \n",
    "    # After first residual\n",
    "    x = x + attn_out\n",
    "    intermediates['after_residual1'] = x.clone()\n",
    "    \n",
    "    # After second LayerNorm\n",
    "    ln2_out = block.ln2(x)\n",
    "    intermediates['after_ln2'] = ln2_out.clone()\n",
    "    \n",
    "    # After FFN\n",
    "    ffn_out = block.ffn(ln2_out)\n",
    "    intermediates['after_ffn'] = ffn_out.clone()\n",
    "    \n",
    "    # After second residual\n",
    "    x = x + ffn_out\n",
    "    intermediates['output'] = x.clone()\n",
    "    \n",
    "    return intermediates\n",
    "\n",
    "# Get intermediate values\n",
    "intermediates = forward_with_intermediates(block, x, mask)\n",
    "\n",
    "# Plot statistics\n",
    "stages = list(intermediates.keys())\n",
    "means = [intermediates[stage].mean().item() for stage in stages]\n",
    "stds = [intermediates[stage].std().item() for stage in stages]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Mean values\n",
    "ax1.plot(means, marker='o', linewidth=2, markersize=8)\n",
    "ax1.set_xticks(range(len(stages)))\n",
    "ax1.set_xticklabels(stages, rotation=45, ha='right')\n",
    "ax1.set_ylabel('Mean', fontsize=12)\n",
    "ax1.set_title('Mean Activation Through Transformer Block', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Standard deviation\n",
    "ax2.plot(stds, marker='s', linewidth=2, markersize=8, color='orange')\n",
    "ax2.set_xticks(range(len(stages)))\n",
    "ax2.set_xticklabels(stages, rotation=45, ha='right')\n",
    "ax2.set_ylabel('Standard Deviation', fontsize=12)\n",
    "ax2.set_title('Std Activation Through Transformer Block', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- LayerNorm keeps activations normalized (mean ≈ 0, std ≈ 1)\")\n",
    "print(\"- Residual connections add back un-normalized values\")\n",
    "print(\"- This pattern repeats for attention and FFN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stacking Transformer Blocks\n",
    "\n",
    "Real models stack **many transformer blocks** to increase capacity:\n",
    "- GPT-2 Small: 12 blocks\n",
    "- GPT-2 Medium: 24 blocks\n",
    "- GPT-2 Large: 36 blocks\n",
    "- GPT-3: 96 blocks!\n",
    "\n",
    "Let's build a stack of blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stack of transformer blocks\n",
    "n_layers = 6\n",
    "d_model = 128\n",
    "n_heads = 8\n",
    "d_ff = 512\n",
    "\n",
    "blocks = nn.ModuleList([\n",
    "    TransformerBlock(\n",
    "        d_model=d_model,\n",
    "        n_heads=n_heads,\n",
    "        d_ff=d_ff,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    for _ in range(n_layers)\n",
    "])\n",
    "\n",
    "print(f\"Transformer Stack:\")\n",
    "print(f\"  Number of layers: {n_layers}\")\n",
    "print(f\"  Model dimension: {d_model}\")\n",
    "print(f\"  Attention heads: {n_heads}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in blocks.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Parameters per block: {total_params // n_layers:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Forward Pass Through Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "mask = create_causal_mask(seq_len)\n",
    "\n",
    "# Pass through all blocks\n",
    "blocks.eval()  # Set to eval mode\n",
    "layer_outputs = [x]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, block in enumerate(blocks):\n",
    "        x, _ = block(x, mask=mask)\n",
    "        layer_outputs.append(x.clone())\n",
    "        print(f\"Layer {i+1}: mean={x.mean():.4f}, std={x.std():.4f}\")\n",
    "\n",
    "print(f\"\\nFinal output shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Visualizing Representations Across Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how representations change across layers\n",
    "# For first sequence in batch, first position\n",
    "position_idx = 0\n",
    "representations = torch.stack([out[0, position_idx] for out in layer_outputs])  # (n_layers+1, d_model)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(representations.T.numpy(), aspect='auto', cmap='RdBu_r', vmin=-2, vmax=2)\n",
    "plt.colorbar(label='Activation Value')\n",
    "plt.xlabel('Layer', fontsize=12)\n",
    "plt.ylabel('Feature Dimension', fontsize=12)\n",
    "plt.title(f'Representation Evolution Across Layers (Position {position_idx})', fontsize=14)\n",
    "plt.xticks(range(n_layers + 1), ['Input'] + [f'L{i+1}' for i in range(n_layers)])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each column shows the feature vector after a transformer layer.\")\n",
    "print(\"Notice how the representation evolves as we go deeper!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Understanding Capacity and Depth\n",
    "\n",
    "Let's explore how depth affects model capacity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(d_model: int, n_heads: int, d_ff: int, n_layers: int) -> int:\n",
    "    \"\"\"Count parameters in a stack of transformer blocks.\"\"\"\n",
    "    # Single block parameters\n",
    "    # Attention: 4 projections (Q, K, V, O) each d_model × d_model\n",
    "    attn_params = 4 * d_model * d_model + 4 * d_model  # weights + biases\n",
    "    \n",
    "    # FFN: two projections\n",
    "    ffn_params = (d_model * d_ff + d_ff) + (d_ff * d_model + d_model)\n",
    "    \n",
    "    # LayerNorm: 2 sets of (scale + shift)\n",
    "    ln_params = 2 * (d_model + d_model)\n",
    "    \n",
    "    block_params = attn_params + ffn_params + ln_params\n",
    "    return block_params * n_layers\n",
    "\n",
    "# Compare different configurations\n",
    "configs = [\n",
    "    {\"name\": \"Tiny\", \"d_model\": 64, \"n_heads\": 4, \"d_ff\": 256, \"n_layers\": 4},\n",
    "    {\"name\": \"Small\", \"d_model\": 128, \"n_heads\": 8, \"d_ff\": 512, \"n_layers\": 6},\n",
    "    {\"name\": \"Medium\", \"d_model\": 256, \"n_heads\": 8, \"d_ff\": 1024, \"n_layers\": 12},\n",
    "    {\"name\": \"Large\", \"d_model\": 512, \"n_heads\": 16, \"d_ff\": 2048, \"n_layers\": 24},\n",
    "]\n",
    "\n",
    "print(f\"{'Config':<10} {'d_model':<10} {'n_heads':<10} {'d_ff':<10} {'n_layers':<10} {'Parameters':<15}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for config in configs:\n",
    "    params = count_parameters(\n",
    "        config[\"d_model\"],\n",
    "        config[\"n_heads\"],\n",
    "        config[\"d_ff\"],\n",
    "        config[\"n_layers\"]\n",
    "    )\n",
    "    print(f\"{config['name']:<10} {config['d_model']:<10} {config['n_heads']:<10} \"\n",
    "          f\"{config['d_ff']:<10} {config['n_layers']:<10} {params:>12,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "Let's recap what we've learned about transformer blocks:\n",
    "\n",
    "1. **Transformer block = Attention + FFN**:\n",
    "   - Multi-head attention captures relationships\n",
    "   - Feedforward network adds capacity\n",
    "   - Both use residual connections and layer normalization\n",
    "\n",
    "2. **Residual connections enable deep networks**:\n",
    "   - Direct path for gradients\n",
    "   - Prevents vanishing gradients\n",
    "   - Essential for training 100+ layer models\n",
    "\n",
    "3. **Layer normalization stabilizes training**:\n",
    "   - Normalizes across features (not batch)\n",
    "   - Works well with variable-length sequences\n",
    "   - Allows higher learning rates\n",
    "\n",
    "4. **Pre-norm architecture is modern best practice**:\n",
    "   - Better gradient flow than post-norm\n",
    "   - More stable training\n",
    "   - Used in GPT-2, GPT-3, and beyond\n",
    "\n",
    "5. **Feedforward network expands and contracts**:\n",
    "   - Typically 4× expansion (d_model → 4 * d_model → d_model)\n",
    "   - GELU activation for smooth gradients\n",
    "   - Processes each position independently\n",
    "\n",
    "6. **Stacking blocks increases capacity**:\n",
    "   - More layers = more complex patterns\n",
    "   - GPT-3 has 96 transformer blocks!\n",
    "   - Parameters grow linearly with depth\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that we understand transformer blocks, we're ready to build the **complete GPT model** - adding embeddings, positional encoding, and the output layer!\n",
    "\n",
    "Continue to **Notebook 04: Complete GPT Model** →"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017)\n",
    "- [Deep Residual Learning](https://arxiv.org/abs/1512.03385) (He et al., 2015)\n",
    "- [Layer Normalization](https://arxiv.org/abs/1607.06450) (Ba et al., 2016)\n",
    "- [On Layer Normalization in Transformers](https://arxiv.org/abs/2002.04745) (Pre-norm vs Post-norm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Training and Text Generation\n",
    "\n",
    "Welcome to the final notebook in our LLM from Scratch series! In this chapter, we'll learn how to **train** our GPT model and use it for **text generation**.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Training loop fundamentals**: Gradient descent, backpropagation, optimization\n",
    "2. **Learning rate scheduling**: Warmup and cosine decay\n",
    "3. **Gradient accumulation**: Training with larger effective batch sizes\n",
    "4. **Training best practices**: Checkpointing, validation, monitoring\n",
    "5. **Text generation strategies**: Greedy, sampling, top-k, top-p, beam search\n",
    "6. **Advanced generation**: Temperature, repetition penalty, n-gram blocking\n",
    "7. **Hands-on training**: Train a small GPT model on real data\n",
    "\n",
    "This is where we see our model come to life!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training Fundamentals\n",
    "\n",
    "Training a language model means teaching it to **predict the next token**.\n",
    "\n",
    "### The Training Objective:\n",
    "\n",
    "Given text: \"The cat sat on the mat\"\n",
    "\n",
    "```\n",
    "Input:  [The] [cat] [sat] [on] [the]\n",
    "Target: [cat] [sat] [on] [the] [mat]\n",
    "```\n",
    "\n",
    "At each position, predict the **next token**.\n",
    "\n",
    "### Loss Function: Cross-Entropy\n",
    "\n",
    "$$\\text{Loss} = -\\frac{1}{N}\\sum_{i=1}^{N} \\log P(\\text{target}_i | \\text{input}_{<i})$$\n",
    "\n",
    "- **Lower loss** = better predictions\n",
    "- **Perplexity** = exp(loss) = \"average branching factor\"\n",
    "- Perfect model: loss = 0, perplexity = 1\n",
    "- Random model: loss ≈ log(vocab_size) ≈ 10.8, perplexity ≈ 50,000\n",
    "\n",
    "### Training Loop:\n",
    "\n",
    "```python\n",
    "for batch in train_data:\n",
    "    # 1. Forward pass\n",
    "    logits, loss = model(input_ids, targets=targets)\n",
    "    \n",
    "    # 2. Backward pass (compute gradients)\n",
    "    loss.backward()\n",
    "    \n",
    "    # 3. Optimizer step (update weights)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning Rate Scheduling\n",
    "\n",
    "**Learning rate** controls how much to update weights:\n",
    "- Too high: training unstable, doesn't converge\n",
    "- Too low: training slow, gets stuck\n",
    "\n",
    "### Warmup + Cosine Decay (GPT-2/GPT-3 schedule):\n",
    "\n",
    "```\n",
    "Learning Rate\n",
    "    ↑\n",
    "max │     ╱╲\n",
    "    │    ╱  ╲___\n",
    "    │   ╱       ╲___\n",
    "    │  ╱            ╲___\n",
    "min │ ╱                 ╲___\n",
    "    └─────────────────────────→ Steps\n",
    "      ↑         ↑\n",
    "    Warmup    Cosine Decay\n",
    "```\n",
    "\n",
    "### Why This Works:\n",
    "\n",
    "1. **Warmup** (first ~2k steps):\n",
    "   - Start with low LR (lr/warmup_steps)\n",
    "   - Gradually increase to max LR\n",
    "   - Prevents unstable early training\n",
    "\n",
    "2. **Cosine Decay** (rest of training):\n",
    "   - Smoothly decrease from max LR to min LR\n",
    "   - Allows fine-tuning at the end\n",
    "   - Better final performance\n",
    "\n",
    "### Formula:\n",
    "\n",
    "```python\n",
    "if step < warmup_steps:\n",
    "    lr = max_lr * (step + 1) / warmup_steps\n",
    "else:\n",
    "    progress = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "    lr = max_lr * 0.5 * (1 + cos(π * progress))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Accumulation\n",
    "\n",
    "**Problem**: Large batch sizes don't fit in GPU memory.\n",
    "\n",
    "**Solution**: Accumulate gradients over multiple small batches.\n",
    "\n",
    "### Without Gradient Accumulation:\n",
    "```python\n",
    "# Batch size = 32 (may not fit!)\n",
    "loss = model(batch_32)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "### With Gradient Accumulation:\n",
    "```python\n",
    "# Process 4 batches of size 8 = effective batch size 32\n",
    "for i in range(4):\n",
    "    loss = model(batch_8) / 4  # Scale loss\n",
    "    loss.backward()  # Accumulate gradients\n",
    "\n",
    "optimizer.step()  # Update once\n",
    "optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "### Benefits:\n",
    "\n",
    "- ✅ Train with large effective batch sizes\n",
    "- ✅ Fits in limited GPU memory\n",
    "- ✅ Better gradient estimates\n",
    "- ✅ More stable training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Generation Strategies\n",
    "\n",
    "Once trained, we can generate text in multiple ways:\n",
    "\n",
    "### 1. Greedy Decoding (Deterministic)\n",
    "```python\n",
    "next_token = torch.argmax(logits, dim=-1)  # Always pick most likely\n",
    "```\n",
    "- ✅ Fast, deterministic\n",
    "- ❌ Repetitive, boring\n",
    "\n",
    "### 2. Random Sampling\n",
    "```python\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "next_token = torch.multinomial(probs, num_samples=1)\n",
    "```\n",
    "- ✅ Diverse output\n",
    "- ❌ Can be incoherent\n",
    "\n",
    "### 3. Temperature Sampling\n",
    "```python\n",
    "logits = logits / temperature  # Scale before softmax\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "```\n",
    "- Temperature = 1.0: Normal sampling\n",
    "- Temperature < 1.0: More confident (sharper distribution)\n",
    "- Temperature > 1.0: More random (flatter distribution)\n",
    "\n",
    "### 4. Top-K Sampling\n",
    "```python\n",
    "top_k_logits, top_k_indices = torch.topk(logits, k=50)\n",
    "# Sample only from top-k most likely tokens\n",
    "```\n",
    "- ✅ Prevents sampling unlikely tokens\n",
    "- ✅ Good balance of quality and diversity\n",
    "\n",
    "### 5. Top-P (Nucleus) Sampling\n",
    "```python\n",
    "# Sample from smallest set with cumulative probability >= p\n",
    "sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "cumsum = torch.cumsum(sorted_probs, dim=-1)\n",
    "mask = cumsum <= p  # Keep tokens until cumsum reaches p\n",
    "```\n",
    "- ✅ Adaptive vocabulary size\n",
    "- ✅ Better than top-k for varying distributions\n",
    "\n",
    "### 6. Beam Search\n",
    "- Keep N best hypotheses at each step\n",
    "- Explore multiple paths simultaneously\n",
    "- Better for tasks requiring correctness (translation)\n",
    "- More expensive than sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hands-On: Training a GPT Model\n",
    "\n",
    "Let's train a small GPT model on sample data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.llm import (\n",
    "    GPTModel,\n",
    "    ModelConfig,\n",
    "    Trainer,\n",
    "    TrainingConfig,\n",
    "    TextGenerator,\n",
    "    GenerationConfig,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training texts (in practice, use much more data!)\n",
    "train_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Transformers revolutionized natural language processing.\",\n",
    "    \"GPT models use attention mechanisms for text generation.\",\n",
    "    \"Deep learning requires large amounts of training data.\",\n",
    "] * 20  # Repeat for more data\n",
    "\n",
    "val_texts = [\n",
    "    \"Neural networks learn patterns from data.\",\n",
    "    \"Language models predict the next word.\",\n",
    "]\n",
    "\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "print(f\"Validation samples: {len(val_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Tokenize and Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_texts(texts, max_length=64):\n",
    "    \"\"\"Tokenize texts and create input-target pairs.\"\"\"\n",
    "    all_input_ids = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for text in texts:\n",
    "        tokens = tokenizer.encode(text)\n",
    "        \n",
    "        # Skip if too short\n",
    "        if len(tokens) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Truncate or pad to max_length\n",
    "        if len(tokens) > max_length:\n",
    "            tokens = tokens[:max_length]\n",
    "        \n",
    "        # For language modeling: input = tokens[:-1], target = tokens[1:]\n",
    "        # We'll just use same sequence for simplicity\n",
    "        all_input_ids.append(tokens)\n",
    "        all_targets.append(tokens)\n",
    "    \n",
    "    return all_input_ids, all_targets\n",
    "\n",
    "train_inputs, train_targets = tokenize_texts(train_texts)\n",
    "val_inputs, val_targets = tokenize_texts(val_texts)\n",
    "\n",
    "print(f\"Sample input: {train_inputs[0]}\")\n",
    "print(f\"Decoded: {tokenizer.decode(train_inputs[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to same length\n",
    "def pad_sequences(sequences, pad_value=0):\n",
    "    \"\"\"Pad sequences to same length.\"\"\"\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        padded.append(seq + [pad_value] * (max_len - len(seq)))\n",
    "    return torch.tensor(padded)\n",
    "\n",
    "train_input_ids = pad_sequences(train_inputs)\n",
    "train_target_ids = pad_sequences(train_targets)\n",
    "val_input_ids = pad_sequences(val_inputs)\n",
    "val_target_ids = pad_sequences(val_targets)\n",
    "\n",
    "print(f\"Train input shape: {train_input_ids.shape}\")\n",
    "print(f\"Val input shape: {val_input_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and data loaders\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for DataLoader.\"\"\"\n",
    "    input_ids = torch.stack([item[0] for item in batch])\n",
    "    targets = torch.stack([item[1] for item in batch])\n",
    "    return {\"input_ids\": input_ids, \"targets\": targets}\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_target_ids)\n",
    "val_dataset = TensorDataset(val_input_ids, val_target_ids)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Create Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tiny model for fast training\n",
    "model_config = ModelConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seq_len=64,\n",
    "    d_model=128,\n",
    "    n_layers=4,\n",
    "    n_heads=4,\n",
    "    d_ff=512,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "model = GPTModel(model_config)\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "\n",
    "# Create optimizer (AdamW is standard for transformers)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "print(\"Model and optimizer created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training configuration\n",
    "train_config = TrainingConfig(\n",
    "    num_epochs=10,\n",
    "    learning_rate=3e-4,\n",
    "    warmup_steps=50,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_grad_norm=1.0,\n",
    "    log_interval=5,\n",
    "    eval_interval=20,\n",
    "    save_interval=50,\n",
    "    device=\"cpu\",  # Use CPU for this demo\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {train_config.num_epochs}\")\n",
    "print(f\"  Learning rate: {train_config.learning_rate}\")\n",
    "print(f\"  Warmup steps: {train_config.warmup_steps}\")\n",
    "print(f\"  Gradient accumulation: {train_config.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    config=train_config,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    checkpoint_dir=\"../checkpoints\"\n",
    ")\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "# Train\n",
    "stats = trainer.train()\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Final loss: {stats['final_train_loss']:.4f}\")\n",
    "print(f\"Best validation loss: {stats['best_val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training loss\n",
    "ax1.plot(trainer.metrics_history['step'], trainer.metrics_history['train_loss'], label='Train Loss')\n",
    "if trainer.metrics_history['val_loss']:\n",
    "    val_steps = [trainer.metrics_history['step'][i] for i in range(0, len(trainer.metrics_history['step']), len(trainer.metrics_history['step']) // len(trainer.metrics_history['val_loss']))]\n",
    "    ax1.plot(val_steps[:len(trainer.metrics_history['val_loss'])], trainer.metrics_history['val_loss'], label='Val Loss', marker='o')\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "ax2.plot(trainer.metrics_history['step'], trainer.metrics_history['learning_rate'])\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('Learning Rate')\n",
    "ax2.set_title('Learning Rate Schedule')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text Generation with Trained Model\n",
    "\n",
    "Now let's generate text with our trained model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Simple Generation (Built-in Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with built-in method\n",
    "prompt = \"Machine learning\"\n",
    "input_ids = tokenizer.encode(prompt)\n",
    "input_tensor = torch.tensor([input_ids])\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "for temp in [0.5, 1.0, 1.5]:\n",
    "    generated = model.generate(\n",
    "        input_tensor,\n",
    "        max_new_tokens=15,\n",
    "        temperature=temp,\n",
    "        top_k=50\n",
    "    )\n",
    "    \n",
    "    text = tokenizer.decode(generated[0].tolist())\n",
    "    print(f\"Temperature {temp}: {text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Advanced Generation (TextGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text generator with different configurations\n",
    "configs = [\n",
    "    {\"name\": \"Greedy\", \"config\": GenerationConfig(max_new_tokens=20, do_sample=False)},\n",
    "    {\"name\": \"Top-K\", \"config\": GenerationConfig(max_new_tokens=20, do_sample=True, top_k=50, temperature=0.8)},\n",
    "    {\"name\": \"Top-P\", \"config\": GenerationConfig(max_new_tokens=20, do_sample=True, top_p=0.9, temperature=0.8)},\n",
    "    {\"name\": \"Beam Search\", \"config\": GenerationConfig(max_new_tokens=20, num_beams=3)},\n",
    "]\n",
    "\n",
    "prompt = \"The transformer\"\n",
    "input_ids = tokenizer.encode(prompt)\n",
    "input_tensor = torch.tensor([input_ids])\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "for cfg in configs:\n",
    "    generator = TextGenerator(model, cfg[\"config\"])\n",
    "    output = generator.generate(input_tensor)\n",
    "    text = tokenizer.decode(output[0].tolist())\n",
    "    print(f\"{cfg['name']}: {text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Comparing Sampling Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple samples with same prompt\n",
    "prompt = \"Deep learning\"\n",
    "input_ids = tokenizer.encode(prompt)\n",
    "input_tensor = torch.tensor([input_ids])\n",
    "\n",
    "config = GenerationConfig(\n",
    "    max_new_tokens=15,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "generator = TextGenerator(model, config)\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"Generated variations (sampling):\")\n",
    "for i in range(5):\n",
    "    output = generator.generate(input_tensor)\n",
    "    text = tokenizer.decode(output[0].tolist())\n",
    "    print(f\"{i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced: Repetition Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without repetition penalty\n",
    "config_no_penalty = GenerationConfig(\n",
    "    max_new_tokens=30,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.0,  # No penalty\n",
    ")\n",
    "\n",
    "# With repetition penalty\n",
    "config_with_penalty = GenerationConfig(\n",
    "    max_new_tokens=30,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.2,  # Penalize repeats\n",
    ")\n",
    "\n",
    "prompt = \"Artificial intelligence\"\n",
    "input_ids = tokenizer.encode(prompt)\n",
    "input_tensor = torch.tensor([input_ids])\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "print(\"WITHOUT repetition penalty:\")\n",
    "gen1 = TextGenerator(model, config_no_penalty)\n",
    "output1 = gen1.generate(input_tensor)\n",
    "print(tokenizer.decode(output1[0].tolist()))\n",
    "\n",
    "print(\"\\nWITH repetition penalty (1.2):\")\n",
    "gen2 = TextGenerator(model, config_with_penalty)\n",
    "output2 = gen2.generate(input_tensor)\n",
    "print(tokenizer.decode(output2[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "Congratulations on completing the LLM from Scratch series! Let's recap:\n",
    "\n",
    "### Training:\n",
    "\n",
    "1. **Training objective**: Predict next token with cross-entropy loss\n",
    "2. **LR scheduling**: Warmup + cosine decay for stable training\n",
    "3. **Gradient accumulation**: Train with large effective batch sizes\n",
    "4. **Best practices**: Checkpointing, validation, gradient clipping\n",
    "5. **Monitoring**: Track loss, perplexity, learning rate\n",
    "\n",
    "### Generation:\n",
    "\n",
    "1. **Greedy decoding**: Fast but repetitive\n",
    "2. **Sampling**: Diverse but can be incoherent\n",
    "3. **Temperature**: Control randomness (0.5-1.5 typical)\n",
    "4. **Top-K**: Sample from top K tokens (50-100 typical)\n",
    "5. **Top-P (Nucleus)**: Adaptive vocabulary size (0.9-0.95 typical)\n",
    "6. **Beam search**: Multiple hypotheses, better quality\n",
    "7. **Repetition penalty**: Reduce repetitive text (1.1-1.5 typical)\n",
    "\n",
    "### What We Built:\n",
    "\n",
    "1. **Chapter 1**: Tokenization (BPE encoding)\n",
    "2. **Chapter 2**: Attention mechanism (multi-head self-attention)\n",
    "3. **Chapter 3**: Transformer blocks (attention + FFN)\n",
    "4. **Chapter 4**: Complete GPT model (embeddings + transformer + output)\n",
    "5. **Chapter 5**: Training and generation (this chapter!)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To go further:\n",
    "\n",
    "1. **More data**: Train on larger datasets (Wikipedia, books, web text)\n",
    "2. **Bigger models**: Scale up to GPT-2/GPT-3 sizes\n",
    "3. **Fine-tuning**: Adapt to specific tasks (instruction following, chat)\n",
    "4. **Optimization**: Mixed precision (FP16), distributed training\n",
    "5. **Advanced techniques**: RLHF, PPO, DPO for alignment\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've built a complete GPT-style language model from scratch and understand:\n",
    "- Tokenization and embeddings\n",
    "- Attention mechanisms\n",
    "- Transformer architecture\n",
    "- Training procedures\n",
    "- Text generation strategies\n",
    "\n",
    "This is the foundation that powers ChatGPT, GPT-4, and other modern LLMs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "### Training:\n",
    "- [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)\n",
    "- [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101) (AdamW)\n",
    "- [On Layer Normalization in Transformers](https://arxiv.org/abs/2002.04745)\n",
    "\n",
    "### Generation:\n",
    "- [The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751) (Nucleus sampling)\n",
    "- [Hierarchical Neural Story Generation](https://arxiv.org/abs/1805.04833) (Top-k sampling)\n",
    "- [CTRL: A Conditional Transformer Language Model](https://arxiv.org/abs/1909.05858) (Repetition penalty)\n",
    "\n",
    "### Advanced:\n",
    "- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) (InstructGPT)\n",
    "- [Constitutional AI](https://arxiv.org/abs/2212.08073) (Anthropic's approach)\n",
    "- [Direct Preference Optimization](https://arxiv.org/abs/2305.18290) (DPO)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

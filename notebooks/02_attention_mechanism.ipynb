{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: The Attention Mechanism\n",
    "\n",
    "Welcome to the second notebook in our LLM from Scratch series! In this chapter, we'll explore the **attention mechanism** - the revolutionary innovation that powers modern transformer models.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **What is attention** and why it's revolutionary\n",
    "2. **Scaled dot-product attention**: The core formula\n",
    "3. **Multi-head attention**: Attending to multiple representation subspaces\n",
    "4. **Causal masking**: Essential for autoregressive generation\n",
    "5. **Hands-on visualization** of attention patterns\n",
    "6. **Implementation details** from our codebase\n",
    "\n",
    "This is the heart of the transformer architecture - let's understand it deeply!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Attention?\n",
    "\n",
    "**Attention** allows each position in a sequence to look at other positions and gather relevant information.\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "When processing a word, we want to look at **all other words** in the sentence to understand context:\n",
    "\n",
    "```\n",
    "Sentence: \"The cat sat on the mat\"\n",
    "\n",
    "When processing \"cat\":\n",
    "- Look at \"The\" → what kind of cat?\n",
    "- Look at \"sat\" → what is the cat doing?\n",
    "- Look at \"mat\" → where is the cat?\n",
    "```\n",
    "\n",
    "### Why is this Revolutionary?\n",
    "\n",
    "Before transformers, models processed sequences **sequentially** (RNNs/LSTMs):\n",
    "- ❌ Slow (cannot parallelize)\n",
    "- ❌ Limited memory (vanishing gradients)\n",
    "- ❌ Struggles with long-range dependencies\n",
    "\n",
    "Attention processes **all positions in parallel**:\n",
    "- ✅ Fast (massively parallel on GPUs)\n",
    "- ✅ Unlimited memory (direct connections)\n",
    "- ✅ Captures long-range dependencies easily\n",
    "\n",
    "This is why GPT, BERT, and other transformers are so powerful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scaled Dot-Product Attention\n",
    "\n",
    "The attention mechanism computes three things:\n",
    "\n",
    "### Query, Key, Value (Q, K, V)\n",
    "\n",
    "For each position, we create three vectors:\n",
    "- **Query (Q)**: What I'm looking for\n",
    "- **Key (K)**: What I can offer\n",
    "- **Value (V)**: The actual information I contain\n",
    "\n",
    "Think of it like a database lookup:\n",
    "- Query: \"Show me all documents about cats\"\n",
    "- Keys: Document tags/titles\n",
    "- Values: Document contents\n",
    "\n",
    "### The Formula\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Let's break this down:\n",
    "\n",
    "1. **$QK^T$**: Compute similarity scores between all query-key pairs\n",
    "   - Shape: (seq_len, seq_len)\n",
    "   - Each element [i, j] = how much position i attends to position j\n",
    "\n",
    "2. **$\\frac{1}{\\sqrt{d_k}}$**: Scale by square root of key dimension\n",
    "   - Prevents dot products from growing too large\n",
    "   - Keeps gradients stable (avoids softmax saturation)\n",
    "\n",
    "3. **$\\text{softmax}(...)$**: Convert scores to probabilities\n",
    "   - Each row sums to 1\n",
    "   - Higher scores → higher attention weights\n",
    "\n",
    "4. **$... V$**: Weighted sum of values\n",
    "   - Aggregate information based on attention weights\n",
    "   - Output: contextualized representation for each position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hands-On: Implementing Attention Step-by-Step\n",
    "\n",
    "Let's implement attention from scratch to understand each component!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Simple Attention from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries of shape (batch, seq_len, d_k)\n",
    "        K: Keys of shape (batch, seq_len, d_k)\n",
    "        V: Values of shape (batch, seq_len, d_v)\n",
    "    \n",
    "    Returns:\n",
    "        Output and attention weights\n",
    "    \"\"\"\n",
    "    # Step 1: Compute attention scores\n",
    "    d_k = Q.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch, seq_len, seq_len)\n",
    "    \n",
    "    # Step 2: Scale by sqrt(d_k)\n",
    "    scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    \n",
    "    # Step 3: Apply softmax\n",
    "    attn_weights = F.softmax(scores, dim=-1)  # (batch, seq_len, seq_len)\n",
    "    \n",
    "    # Step 4: Apply attention to values\n",
    "    output = torch.matmul(attn_weights, V)  # (batch, seq_len, d_v)\n",
    "    \n",
    "    return output, attn_weights\n",
    "\n",
    "# Test with simple example\n",
    "batch_size = 1\n",
    "seq_len = 4\n",
    "d_model = 8\n",
    "\n",
    "# Create random Q, K, V\n",
    "Q = torch.randn(batch_size, seq_len, d_model)\n",
    "K = torch.randn(batch_size, seq_len, d_model)\n",
    "V = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attn_weights = simple_attention(Q, K, V)\n",
    "\n",
    "print(f\"Input shapes:\")\n",
    "print(f\"  Q: {Q.shape}\")\n",
    "print(f\"  K: {K.shape}\")\n",
    "print(f\"  V: {V.shape}\")\n",
    "print(f\"\\nOutput shapes:\")\n",
    "print(f\"  Output: {output.shape}\")\n",
    "print(f\"  Attention weights: {attn_weights.shape}\")\n",
    "print(f\"\\nAttention weights (each row sums to 1):\")\n",
    "print(attn_weights[0])\n",
    "print(f\"\\nRow sums: {attn_weights[0].sum(dim=-1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualizing Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention_weights: torch.Tensor, title: str = \"Attention Weights\") -> None:\n",
    "    \"\"\"\n",
    "    Visualize attention weights as a heatmap.\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: Tensor of shape (seq_len, seq_len)\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        attention_weights.detach().cpu().numpy(),\n",
    "        annot=True,\n",
    "        fmt=\".3f\",\n",
    "        cmap=\"YlOrRd\",\n",
    "        cbar=True,\n",
    "        square=True,\n",
    "        xticklabels=[f\"Pos {i}\" for i in range(attention_weights.size(0))],\n",
    "        yticklabels=[f\"Pos {i}\" for i in range(attention_weights.size(0))],\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Key Position (attending TO)\")\n",
    "    plt.ylabel(\"Query Position (attending FROM)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the attention pattern\n",
    "plot_attention(\n",
    "    attn_weights[0],\n",
    "    title=\"Attention Pattern (No Masking)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Heatmap:\n",
    "\n",
    "- **Rows**: Query positions (which position is attending)\n",
    "- **Columns**: Key positions (which positions are being attended to)\n",
    "- **Values**: Attention weights (0 to 1, each row sums to 1)\n",
    "- **Bright cells**: High attention (this position is important)\n",
    "- **Dark cells**: Low attention (this position is ignored)\n",
    "\n",
    "Each position can look at **all positions** including future ones!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Causal Masking: Essential for Autoregressive Models\n",
    "\n",
    "For language generation (like GPT), we need **causal masking**:\n",
    "- When predicting token at position $i$, we can only look at positions $< i$\n",
    "- Cannot \"cheat\" by looking at future tokens\n",
    "- This makes the model **autoregressive**: generates one token at a time\n",
    "\n",
    "### Causal Mask Pattern\n",
    "\n",
    "```\n",
    "Position:  0  1  2  3\n",
    "        0 [✓  ✗  ✗  ✗]  ← Can only see position 0\n",
    "        1 [✓  ✓  ✗  ✗]  ← Can see positions 0, 1\n",
    "        2 [✓  ✓  ✓  ✗]  ← Can see positions 0, 1, 2\n",
    "        3 [✓  ✓  ✓  ✓]  ← Can see all positions\n",
    "```\n",
    "\n",
    "This creates a **lower triangular** pattern!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our causal mask function\n",
    "from src.llm.attention import create_causal_mask\n",
    "\n",
    "# Create a causal mask\n",
    "seq_len = 6\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "print(f\"Causal mask for sequence length {seq_len}:\")\n",
    "print(causal_mask.int())\n",
    "print(f\"\\nTrue = masked (cannot attend), False = unmasked (can attend)\")\n",
    "\n",
    "# Visualize the mask\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    causal_mask.int().numpy(),\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"RdYlGn_r\",\n",
    "    cbar=False,\n",
    "    square=True,\n",
    "    xticklabels=[f\"Pos {i}\" for i in range(seq_len)],\n",
    "    yticklabels=[f\"Pos {i}\" for i in range(seq_len)],\n",
    ")\n",
    "plt.title(\"Causal Mask (1 = Masked, 0 = Visible)\")\n",
    "plt.xlabel(\"Key Position\")\n",
    "plt.ylabel(\"Query Position\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Causal Mask to Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention with causal masking.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, d_k = Q.shape\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    \n",
    "    # Apply causal mask\n",
    "    mask = create_causal_mask(seq_len, device=Q.device)\n",
    "    scores = scores.masked_fill(mask.unsqueeze(0), -1e9)  # Set masked positions to very negative\n",
    "    \n",
    "    # Softmax and apply to values\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "    \n",
    "    return output, attn_weights\n",
    "\n",
    "# Test causal attention\n",
    "output_causal, attn_weights_causal = causal_attention(Q, K, V)\n",
    "\n",
    "print(f\"Causal attention weights:\")\n",
    "print(attn_weights_causal[0])\n",
    "\n",
    "# Visualize\n",
    "plot_attention(\n",
    "    attn_weights_causal[0],\n",
    "    title=\"Attention Pattern WITH Causal Masking\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "\n",
    "Notice the **lower triangular pattern**:\n",
    "- Top-right triangle is all zeros (future tokens masked)\n",
    "- Each position only attends to itself and previous positions\n",
    "- This is essential for autoregressive language generation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Head Attention\n",
    "\n",
    "**Multi-head attention** runs attention multiple times in parallel:\n",
    "- Each \"head\" learns different attention patterns\n",
    "- Head 1 might focus on syntax\n",
    "- Head 2 might focus on semantics\n",
    "- Head 3 might focus on long-range dependencies\n",
    "\n",
    "### How it Works:\n",
    "\n",
    "1. **Split** $d_{model}$ into $n_{heads}$ smaller dimensions\n",
    "2. **Run attention** on each head independently\n",
    "3. **Concatenate** all head outputs\n",
    "4. **Project** back to $d_{model}$\n",
    "\n",
    "### Example:\n",
    "\n",
    "```\n",
    "d_model = 512, n_heads = 8\n",
    "→ Each head dimension = 512 / 8 = 64\n",
    "\n",
    "Input: (batch, seq_len, 512)\n",
    "  ↓ Split into 8 heads\n",
    "Heads: 8 × (batch, seq_len, 64)\n",
    "  ↓ Run attention on each head\n",
    "Outputs: 8 × (batch, seq_len, 64)\n",
    "  ↓ Concatenate\n",
    "Concat: (batch, seq_len, 512)\n",
    "  ↓ Output projection\n",
    "Final: (batch, seq_len, 512)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using Our Multi-Head Attention Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm.attention import MultiHeadAttention\n",
    "\n",
    "# Create multi-head attention module\n",
    "d_model = 128\n",
    "n_heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model=d_model, n_heads=n_heads, dropout=0.0)\n",
    "\n",
    "print(f\"Multi-Head Attention Configuration:\")\n",
    "print(f\"  d_model: {d_model}\")\n",
    "print(f\"  n_heads: {n_heads}\")\n",
    "print(f\"  head_dim: {mha.head_dim}\")\n",
    "print(f\"  scale factor: {mha.scale:.4f}\")\n",
    "\n",
    "# Create input\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Forward pass without mask\n",
    "output, _ = mha(x, return_attention=False)\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Shape preserved: {x.shape == output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Visualizing Multi-Head Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention weights for all heads\n",
    "mask = create_causal_mask(seq_len)\n",
    "output, attn_weights = mha(x, mask=mask, return_attention=True)\n",
    "\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"  (batch_size, n_heads, seq_len, seq_len)\")\n",
    "\n",
    "# Visualize first 4 heads for the first batch item\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle(\"Attention Patterns Across Different Heads\", fontsize=16)\n",
    "\n",
    "for head_idx in range(8):\n",
    "    row = head_idx // 4\n",
    "    col = head_idx % 4\n",
    "    \n",
    "    ax = axes[row, col]\n",
    "    head_attn = attn_weights[0, head_idx].detach().cpu().numpy()\n",
    "    \n",
    "    im = ax.imshow(head_attn, cmap=\"YlOrRd\", aspect=\"auto\")\n",
    "    ax.set_title(f\"Head {head_idx + 1}\")\n",
    "    ax.set_xlabel(\"Key Position\")\n",
    "    ax.set_ylabel(\"Query Position\")\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "Each head learns **different attention patterns**:\n",
    "- Some heads might focus on **nearby positions** (local context)\n",
    "- Some heads might focus on **distant positions** (long-range dependencies)\n",
    "- Some heads might attend **uniformly** (global context)\n",
    "- All heads respect the **causal mask** (lower triangular)\n",
    "\n",
    "This diversity allows the model to capture different types of relationships!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practical Example: Sentence Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm import Tokenizer\n",
    "\n",
    "# Create tokenizer and encode a sentence\n",
    "tokenizer = Tokenizer()\n",
    "text = \"The cat sat on the mat\"\n",
    "token_ids = tokenizer.encode(text)\n",
    "tokens = [tokenizer.decode([tid]) for tid in token_ids]\n",
    "\n",
    "print(f\"Sentence: {text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "\n",
    "# Create embeddings (simplified - real model learns these)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embedding = torch.nn.Embedding(vocab_size, d_model)\n",
    "token_tensor = torch.tensor([token_ids])\n",
    "x = embedding(token_tensor)\n",
    "\n",
    "print(f\"\\nEmbedding shape: {x.shape}\")\n",
    "\n",
    "# Apply multi-head attention with causal mask\n",
    "seq_len = len(token_ids)\n",
    "mask = create_causal_mask(seq_len)\n",
    "output, attn_weights = mha(x, mask=mask, return_attention=True)\n",
    "\n",
    "# Visualize attention for head 0\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    attn_weights[0, 0].detach().cpu().numpy(),\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"YlOrRd\",\n",
    "    xticklabels=tokens,\n",
    "    yticklabels=tokens,\n",
    "    square=True,\n",
    ")\n",
    "plt.title(f'Attention Pattern for: \"{text}\" (Head 1)')\n",
    "plt.xlabel(\"Attending TO\")\n",
    "plt.ylabel(\"Attending FROM\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Each row shows where that token is looking\")\n",
    "print(\"- Higher values mean stronger attention\")\n",
    "print(\"- Causal mask ensures we only look at past tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Implementation Details\n",
    "\n",
    "Let's examine some important implementation details from our codebase:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Why Scale by $\\sqrt{d_k}$?\n",
    "\n",
    "Without scaling, dot products can become very large, causing softmax to saturate:\n",
    "\n",
    "```python\n",
    "# Without scaling\n",
    "scores = Q @ K.T  # Can be very large!\n",
    "probs = softmax(scores)  # Gradients → 0 (vanishing gradients)\n",
    "\n",
    "# With scaling\n",
    "scores = (Q @ K.T) / sqrt(d_k)  # Keeps values reasonable\n",
    "probs = softmax(scores)  # Healthy gradients\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the effect of scaling\n",
    "d_k = 64\n",
    "q = torch.randn(1, 10, d_k)\n",
    "k = torch.randn(1, 10, d_k)\n",
    "\n",
    "scores_unscaled = torch.matmul(q, k.transpose(-2, -1))\n",
    "scores_scaled = scores_unscaled / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "print(f\"Unscaled scores - Mean: {scores_unscaled.mean():.2f}, Std: {scores_unscaled.std():.2f}\")\n",
    "print(f\"Scaled scores   - Mean: {scores_scaled.mean():.2f}, Std: {scores_scaled.std():.2f}\")\n",
    "print(f\"\\nScaling factor: 1/√{d_k} = {1/torch.sqrt(torch.tensor(d_k, dtype=torch.float32)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Handling NaN from Fully Masked Rows\n",
    "\n",
    "When an entire row is masked (e.g., padding tokens), softmax can produce NaN:\n",
    "\n",
    "```python\n",
    "# Problem\n",
    "scores = [-inf, -inf, -inf]  # All masked\n",
    "probs = softmax(scores)      # [NaN, NaN, NaN]\n",
    "\n",
    "# Solution in our code\n",
    "attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "attn_weights = torch.nan_to_num(attn_weights, nan=0.0)  # Replace NaN with 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Dropout in Attention\n",
    "\n",
    "Our implementation applies dropout to:\n",
    "1. **Attention weights** (after softmax)\n",
    "2. **Output** (after final projection)\n",
    "\n",
    "```python\n",
    "attn_weights = self.attn_dropout(attn_weights)  # Regularize attention\n",
    "output = self.out_dropout(self.out_proj(attn_output))  # Regularize output\n",
    "```\n",
    "\n",
    "This acts as a regularizer and prevents overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "Let's recap what we've learned:\n",
    "\n",
    "1. **Attention allows parallel processing** of sequences:\n",
    "   - Each position looks at all other positions\n",
    "   - No sequential dependency like RNNs\n",
    "   - This is why transformers are so fast!\n",
    "\n",
    "2. **Scaled dot-product attention** formula:\n",
    "   $$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "   - Query-Key similarity → attention weights\n",
    "   - Attention weights → weighted sum of Values\n",
    "\n",
    "3. **Causal masking** is essential for autoregressive models:\n",
    "   - Prevents attending to future tokens\n",
    "   - Creates lower triangular attention pattern\n",
    "   - Enables left-to-right generation\n",
    "\n",
    "4. **Multi-head attention** learns diverse patterns:\n",
    "   - Multiple heads = multiple attention patterns\n",
    "   - Different heads capture different relationships\n",
    "   - Concatenate and project back to $d_{model}$\n",
    "\n",
    "5. **Implementation details matter**:\n",
    "   - Scale by $\\sqrt{d_k}$ for gradient stability\n",
    "   - Handle NaN from fully masked rows\n",
    "   - Apply dropout for regularization\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand attention, we're ready to build the **Transformer Block** - the complete building block that combines attention with feedforward networks!\n",
    "\n",
    "Continue to **Notebook 03: Transformer Blocks** →"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Original Transformer paper)\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) (Jay Alammar)\n",
    "- [Visualizing Attention in Transformer Models](https://arxiv.org/abs/1904.02679)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
